@article{10.1257/pandp.108.745,
  title = {Report of the {{Search Committee}} to {{Appoint}} a {{Data Editor}} for the {{AEA}}},
  author = {Duflo, Esther and Hoynes, Hilary},
  year = {2018},
  journal = {AEA Papers and Proceedings},
  volume = {108},
  pages = {745},
  issn = {2574-0768},
  doi = {10.1257/pandp.108.745},
  url = {http://www.aeaweb.org/articles?id=10.1257/pandp.108.745},
  urldate = {2018-07-22},
  langid = {english}
}

@article{10.1257/pandp.109.718,
  title = {Report by the {{AEA Data Editor}}},
  author = {Vilhuber, Lars},
  year = {2019},
  month = may,
  journal = {AEA Papers and Proceedings},
  volume = {109},
  pages = {718--29},
  issn = {2574-0768, 2574-0776},
  doi = {10.1257/pandp.109.718},
  url = {http://www.aeaweb.org/articles?id=10.1257/pandp.109.718},
  urldate = {2019-09-21},
  copyright = {All rights reserved},
  langid = {english},
  file = {/home/vilhuber/Zotero/storage/BF4UVMXW/2019 - Report by the AEA Data Editor.pdf}
}

@article{10.1257/pandp.110.764,
  title = {Report by the {{AEA Data Editor}}},
  author = {Vilhuber, Lars and Turitto, James and Welch, Keesler},
  year = {2020},
  month = may,
  journal = {AEA Papers and Proceedings},
  volume = {110},
  pages = {764--75},
  issn = {2574-0768, 2574-0776},
  doi = {10.1257/pandp.110.764},
  copyright = {All rights reserved},
  langid = {english}
}

@article{10.1257/pandp.110.dcap,
  title = {Data and Code Availability Policy},
  author = {{American Economic Association}},
  year = {2020},
  month = may,
  journal = {AEA Papers and Proceedings},
  volume = {110},
  pages = {776--78},
  doi = {10.1257/pandp.110.776}
}

@article{10.1257/pandp.111.808,
  title = {Report by the {{AEA Data Editor}}},
  author = {Vilhuber, Lars},
  year = {2021},
  month = may,
  journal = {AEA Papers and Proceedings},
  volume = {111},
  pages = {808--817},
  issn = {2574-0768, 2574-0776},
  doi = {10.1257/pandp.111.808},
  url = {https://pubs.aeaweb.org/doi/10.1257/pandp.111.808},
  urldate = {2021-05-20},
  copyright = {All rights reserved},
  langid = {english},
  file = {/home/vilhuber/Zotero/storage/LDKWWB4Q/2021_Report by the AEA Data Editor.pdf}
}

@article{10.1257/pandp.112.813,
  title = {Report by the {{AEA Data Editor}}},
  author = {Vilhuber, Lars},
  year = {2022},
  month = may,
  journal = {AEA Papers and Proceedings},
  volume = {112},
  pages = {813--23},
  issn = {2574-0768, 2574-0776},
  doi = {10.1257/pandp.112.813},
  copyright = {All rights reserved},
  langid = {english}
}

@article{10.1257/pandp.113.850,
  title = {Report of the {{AEA Data Editor}}},
  author = {Vilhuber, Lars},
  year = {2023},
  month = may,
  journal = {AEA Papers and Proceedings},
  volume = {113},
  pages = {850--863},
  issn = {2574-0768, 2574-0776},
  doi = {10.1257/pandp.113.850},
  url = {https://pubs.aeaweb.org/doi/10.1257/pandp.113.850},
  urldate = {2023-11-26},
  copyright = {All rights reserved},
  langid = {english},
  file = {/home/vilhuber/Zotero/storage/LWZF3BAA/2023 - Report of the AEA Data Editor.pdf}
}

@article{10.1257/pandp.114.878,
  title = {Report of the {{AEA Data Editor}}},
  author = {Vilhuber, Lars},
  year = {2024},
  month = may,
  journal = {AEA Papers and Proceedings},
  volume = {114},
  pages = {878--890},
  issn = {2574-0768, 2574-0776},
  doi = {10.1257/pandp.114.878},
  url = {https://pubs.aeaweb.org/doi/10.1257/pandp.114.878},
  urldate = {2024-08-22},
  langid = {english}
}

@misc{AmericanEconomicAssociation2019,
  title = {Data and {{Code Availability Policy}}},
  author = {{American Economic Association}},
  year = {2019},
  month = jul,
  url = {https://www.aeaweb.org/journals/policies/data-code},
  urldate = {2019-11-19},
  file = {/home/vilhuber/Zotero/storage/EEMNHKI5/data-code.html}
}

@article{andersonReplicationScientificStandards1994,
  title = {Replication and {{Scientific Standards}} in {{Applied Economics A Decade After}} the {{Journal}} of {{Money}}, {{Credit}} and {{Banking Project}}},
  author = {Anderson, Richard G. and Dewald, William G.},
  year = {1994},
  journal = {Federal Reserve Bank of St. Louis Review},
  volume = {76},
  number = {6},
  doi = {10.20955/r.76.79-83},
  url = {https://research.stlouisfed.org/publications/review/1994/11/01/replication-and-scientific-standards-in-applied-economics-a-decade-after-the-journal-of-money-credit-and-banking-project},
  urldate = {2018-07-19}
}

@article{ballYesWeCan2023,
  title = {``{{Yes We Can}}!'': {{A Practical Approach}} to {{Teaching Reproducibility}} to {{Undergraduates}}},
  shorttitle = {``{{Yes We Can}}!''},
  author = {Ball, Richard},
  year = {2023},
  month = jul,
  journal = {Harvard Data Science Review},
  volume = {5},
  number = {3},
  issn = {2644-2353, 688-8513},
  doi = {10.1162/99608f92.9e002f7b},
  url = {https://hdsr.mitpress.mit.edu/pub/mza9ii75/release/1},
  urldate = {2023-08-02},
  langid = {english},
  file = {/home/vilhuber/Zotero/storage/AYQ2UD2D/Ball_2023_“Yes We Can.pdf}
}

@techreport{banerjeePraiseModerationSuggestions2020,
  title = {In {{Praise}} of {{Moderation}}: {{Suggestions}} for the {{Scope}} and {{Use}} of {{Pre-Analysis Plans}} for {{RCTs}} in {{Economics}}},
  shorttitle = {In {{Praise}} of {{Moderation}}},
  author = {Banerjee, Abhijit and Duflo, Esther and Finkelstein, Amy and Katz, Lawrence and Olken, Benjamin and Sautmann, Anja},
  year = {2020},
  month = apr,
  number = {w26993},
  pages = {w26993},
  address = {Cambridge, MA},
  institution = {National Bureau of Economic Research},
  doi = {10.3386/w26993},
  url = {http://www.nber.org/papers/w26993.pdf},
  urldate = {2021-03-29},
  langid = {english}
}

@article{Bell2013-rv,
  title = {How to {{Persuade Journals}} to {{Accept Your Replication Paper}}},
  author = {Bell, Mark and Miller, Nicholas},
  year = {2013},
  url = {https://politicalsciencereplication.wordpress.com/2013/09/11/guest-blog-how-to-persuade-journals-to-accept-your-replication-paper/}
}

@article{bernankeEditorialStatement2004,
  title = {Editorial {{Statement}}},
  author = {Bernanke, Ben S.},
  year = {2004},
  journal = {The American Economic Review},
  volume = {94},
  number = {1},
  eprint = {3592790},
  eprinttype = {jstor},
  pages = {404--404},
  publisher = {American Economic Association},
  issn = {0002-8282},
  url = {https://www.jstor.org/stable/3592790},
  urldate = {2020-09-01}
}

@article{berryAssessingRateReplication2017,
  title = {Assessing the {{Rate}} of {{Replication}} in {{Economics}}},
  author = {Berry, James and Coffman, Lucas C. and Hanley, Douglas and Gihleb, Rania and Wilson, Alistair J.},
  year = {2017},
  month = may,
  journal = {American Economic Review},
  volume = {107},
  number = {5},
  pages = {27--31},
  issn = {0002-8282},
  doi = {10.1257/aer.p20171119},
  url = {https://www.aeaweb.org/articles?id=10.1257/aer.p20171119},
  urldate = {2018-05-22},
  abstract = {We assess the rate of replication for empirical papers in the 2010 American Economic Review. Across 70 empirical papers, we find that 29 percent have 1 or more citation that partially replicates the original result. While only a minority of papers has a published replication, a majority (60 percent) have either a replication, robustness test, or an extension. Surveying authors within the literature, we find substantial uncertainty over the number of extant replications.},
  langid = {english},
  file = {/home/vilhuber/Zotero/storage/JZ2PBVRJ/articles.html}
}

@techreport{Bollen2015,
  type = {Report of the {{Subcommittee}} on {{Replicability}} in {{Science Advisory Committee}} to the {{National Science Foundation Directorate}} for {{Social}}, {{Behavioral}}, and {{Economic Sciences}}},
  title = {Social, {{Behavioral}}, and {{Economic Sciences Perspectives}} on {{Robust}} and {{Reliable Science}}},
  author = {Bollen, Kenneth and Cacioppo, John T. and Kaplan, Robert M. and Krosnick, Jon A. and Olds, James L.},
  year = {2015},
  institution = {National Science Foundation},
  url = {https://www.nsf.gov/sbe/AC_Materials/SBE_Robust_and_Reliable_Research_Report.pdf},
  urldate = {2018-05-20},
  file = {/home/vilhuber/Zotero/storage/VZCI4ZQ4/SBE_Robust_and_Reliable_Research_Report.pdf}
}

@article{butlerPublishingReplicationPackages2023,
  title = {Publishing {{Replication Packages}}: {{Insights From}} the {{Federal Reserve Bank}} of {{Kansas City}}},
  shorttitle = {Publishing {{Replication Packages}}},
  author = {Butler, Courtney R.},
  year = {2023},
  month = jul,
  journal = {Harvard Data Science Review},
  volume = {5},
  number = {3},
  issn = {2644-2353, 688-8513},
  doi = {10.1162/99608f92.aba61304},
  url = {https://hdsr.mitpress.mit.edu/pub/j0ny4jrg/release/1},
  urldate = {2023-08-02},
  langid = {english},
  file = {/home/vilhuber/Zotero/storage/JWRVAC7W/Butler_2023_Publishing Replication Packages.pdf}
}

@article{CamererScience2016,
  title = {Evaluating Replicability of Laboratory Experiments in Economics},
  author = {Camerer, Colin F. and Dreber, Anna and Forsell, Eskil and Ho, Teck-Hua and Huber, J{\"u}rgen and Johannesson, Magnus and Kirchler, Michael and Almenberg, Johan and Altmejd, Adam and Chan, Taizan and Heikensten, Emma and Holzmeister, Felix and Imai, Taisuke and Isaksson, Siri and Nave, Gideon and Pfeiffer, Thomas and Razen, Michael and Wu, Hang},
  year = {2016},
  month = mar,
  journal = {Science},
  pages = {aaf0918},
  issn = {0036-8075, 1095-9203},
  doi = {10.1126/science.aaf0918},
  url = {http://science.sciencemag.org/content/early/2016/03/02/science.aaf0918},
  urldate = {2018-06-04},
  abstract = {The reproducibility of scientific findings has been called into question. To contribute data about reproducibility in economics, we replicate 18 studies published in the American Economic Review and the Quarterly Journal of Economics in 2011-2014. All replications follow predefined analysis plans publicly posted prior to the replications, and have a statistical power of at least 90\% to detect the original effect size at the 5\% significance level. We find a significant effect in the same direction as the original study for 11 replications (61\%); on average the replicated effect size is 66\% of the original. The reproducibility rate varies between 67\% and 78\% for four additional reproducibility indicators, including a prediction market measure of peer beliefs.},
  copyright = {Copyright {\copyright} 2016, American Association for the Advancement of Science},
  langid = {english},
  pmid = {26940865},
  file = {/home/vilhuber/Zotero/storage/4CAT66SL/science.html}
}

@misc{chambersRegisteredReportsStep2014,
  title = {Registered {{Reports}}: {{A}} Step Change in Scientific Publishing},
  shorttitle = {Registered {{Reports}}},
  author = {Chambers, Chris},
  year = {2014},
  journal = {Reviewers' Update},
  url = {https://www.elsevier.com/reviewers-update/story/innovation-in-publishing/registered-reports-a-step-change-in-scientific-publishing},
  urldate = {2018-07-22},
  abstract = {Professor Chris Chambers, Registered Reports Editor of the Elsevier journal Cortex and one of the concept's founders, on how the initiative combats publication bias},
  langid = {english},
  file = {/home/vilhuber/Zotero/storage/E523E6BJ/registered-reports-a-step-change-in-scientific-publishing.html}
}

@article{ChangAm.Econ.Rev.2017,
  title = {A {{Preanalysis Plan}} to {{Replicate Sixty Economics Research Papers That Worked Half}} of the {{Time}}},
  author = {Chang, Andrew C. and Li, Phillip},
  year = {2017},
  month = may,
  journal = {American Economic Review},
  volume = {107},
  number = {5},
  pages = {60--64},
  issn = {0002-8282},
  doi = {10.1257/aer.p20171034},
  url = {https://www.aeaweb.org/articles?id=10.1257/aer.p20171034},
  urldate = {2018-05-22},
  abstract = {We attempted to replicate 67 macroeconomic papers using author-provided data and code files by following a preanalysis plan. Excluding 6 papers that used confidential data, we obtained data and code replication files for 29 of 35 papers (83 percent) that were required to provide such files as a condition of publication, compared to 11 of 26 papers (42 percent) that were not required to provide such files. Also excluding the 2 papers that used software we did not possess, we replicated 29 of 59 papers (49 percent) with assistance from the authors. We conclude with recommendations on improving replication of economics research.},
  langid = {english},
  file = {/home/vilhuber/Zotero/storage/UAU6BFH3/articles.html}
}

@techreport{Christensen2019,
  type = {Preprint},
  title = {Open {{Science Practices}} Are on the {{Rise}}: {{The State}} of {{Social Science}} ({{3S}}) {{Survey}}},
  shorttitle = {Open {{Science Practices}} Are on the {{Rise}}},
  author = {Christensen, Garret and Wang, Zenan and Paluck, Elizabeth Levy and Swanson, Nicholas and Birke, David J. and Miguel, Edward and Littman, Rebecca},
  year = {2019},
  month = oct,
  institution = {MetaArXiv},
  doi = {10.31222/osf.io/5rksu},
  url = {https://osf.io/5rksu},
  urldate = {2019-10-20},
  abstract = {Has there been meaningful movement toward open science practices within the social sciences in recent years? Discussions about changes in practices such as posting data and pre-registering analyses have been marked by controversy---including controversy over the extent to which change has taken place. This study, based on the State of Social Science (3S) Survey, provides the first comprehensive assessment of awareness of, attitudes towards, perceived norms regarding, and adoption of open science practices within a broadly representative sample of scholars from four major social science disciplines: economics, political science, psychology, and sociology. We observe a steep increase in adoption: as of 2017, over 80\% of scholars had used at least one such practice, rising from one quarter a decade earlier. Attitudes toward research transparency are on average similar between older and younger scholars, but the paceof change differs by field and methodology. According with theories of normal science and scientific change, the timing of increases in adoption coincides with technological innovations and institutional policies. Patterns are consistent with most scholars underestimating the trend toward open science in their discipline.}
}

@article{ChristianInt.J.Digit.Curation2018,
  title = {Operationalizing the {{Replication Standard}}: {{A Case Study}} of the {{Data Curation}} and {{Verification Workflow}} for {{Scholarly Journals}}},
  shorttitle = {Operationalizing the {{Replication Standard}}},
  author = {Christian, Thu-Mai and {Lafferty-Hess}, Sophia and Jacoby, William and Carsey, Thomas},
  year = {2018},
  journal = {International Journal of Digital Curation},
  volume = {13},
  number = {1},
  doi = {10.2218/ijdc.v13i1.555},
  url = {https://osf.io/preprints/socarxiv/cfdba/},
  urldate = {2018-06-04},
  abstract = {In response to widespread concerns about the integrity of research published in scholarly journals, several initiatives have emerged that are promoting research transparency through access to data underlying published scientific findings. Journal editors, in particular, have made a commitment to research transparency by issuing data policies that require authors to submit their data, code, and documentation to data repositories to allow for public access to the data. In the case of the American Journal of Political Science (AJPS) Data Replication Policy, the data also must undergo an independent verification process in which materials are reviewed for quality as a condition of final manuscript publication and acceptance.Aware of the specialized expertise of the data archives, AJPS called upon the Odum Institute Data Archive to provide a data review service that performs data curation and verification of replication datasets. This article presents a case study of the collaboration between AJPS and the Odum Institute Data Archive to develop a workflow that bridges manuscript publication and data review processes. The case study describes the challenges and the successes of the workflow integration, and offers lessons learned that may be applied by other data archives that are considering expanding their services to include data curation and verification services to support reproducible research.}
}

@article{clemensMeaningFailedReplications2017,
  title = {The {{Meaning}} of {{Failed Replications}}: {{A Review}} and {{Proposal}}},
  shorttitle = {The {{Meaning}} of {{Failed Replications}}},
  author = {Clemens, Michael A.},
  year = {2017},
  month = feb,
  journal = {Journal of Economic Surveys},
  volume = {31},
  number = {1},
  pages = {326--342},
  issn = {1467-6419},
  doi = {10.1111/joes.12139},
  url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/joes.12139},
  urldate = {2018-05-22},
  abstract = {The welcome rise of replication tests in economics has not been accompanied by a consensus standard for determining what constitutes a replication. A discrepant replication, in current usage of the term, can signal anything from an unremarkable disagreement over methods to scientific incompetence or misconduct. This paper proposes a standard for classifying one study as a replication of some other study. It is a standard that places the burden of proof on a study to demonstrate that it should have obtained identical results to the original, a conservative standard that is already used implicitly by many researchers. It contrasts this standard with decades of unsuccessful attempts to harmonize terminology, and argues that many prominent results described as replication tests should not be described as such. Adopting a conservative standard like this one can improve incentives for researchers, encouraging more and better replication tests.},
  copyright = {{\copyright} 2015 John Wiley \& Sons Ltd},
  langid = {english},
  keywords = {Ethics,Open data,Replication,Robustness,Transparency},
  file = {/home/vilhuber/Zotero/storage/M8XPUJTM/Clemens_2017_The Meaning of Failed Replications.pdf;/home/vilhuber/Zotero/storage/PDBRJ88U/joes.html}
}

@article{coffmanProposalOrganizePromote2017,
  title = {A {{Proposal}} to {{Organize}} and {{Promote Replications}}},
  author = {Coffman, Lucas C. and Niederle, Muriel and Wilson, Alistair J.},
  year = {2017},
  month = may,
  journal = {American Economic Review},
  volume = {107},
  number = {5},
  pages = {41--45},
  issn = {0002-8282},
  doi = {10.1257/aer.p20171122},
  url = {https://www.aeaweb.org/articles?id=10.1257/aer.p20171122},
  urldate = {2018-05-22},
  abstract = {We make a two-pronged proposal to (i) strengthen the incentives for replication work and (ii) better organize and draw attention to the replications that are conducted. First we propose that top journals publish short "replication reports." These reports could summarize novel work replicating an existing high-impact paper, or they could highlight a replication result embedded in a wider-scope published paper. Second, we suggest incentivizing replications with the currency of our profession: citations. Enforcing a norm of citing replication work alongside the original would provide incentives for replications to both authors and journals.},
  langid = {english},
  file = {/home/vilhuber/Zotero/storage/V2WX8ABN/articles.html}
}

@misc{ColeridgeInitiativeAdaintropython2020,
  title = {Coleridge-{{Initiative}}/Ada-Intro-Python},
  year = {2020},
  month = sep,
  url = {https://github.com/Coleridge-Initiative/ada-intro-python},
  urldate = {2021-05-25},
  abstract = {Contribute to Coleridge-Initiative/ada-intro-python development by creating an account on GitHub.},
  copyright = {CC0-1.0 License         ,                      CC0-1.0 License},
  howpublished = {Coleridge Initiative}
}

@techreport{DataCitationSynthesisGroup2014,
  title = {Joint {{Declaration}} of {{Data Citation Principles}}},
  author = {{Data Citation Synthesis Group} and Martone, Maryann},
  year = {2014},
  institution = {Force11},
  doi = {10.25490/a97f-egyk},
  abstract = {Sound, reproducible scholarship rests upon a foundation of     robust, accessible data. For this to be so in practice as well as theory, data must be accorded     due importance in the practice of scholarship and in the enduring scholarly record. In other words,     data should be considered legitimate, citable products of research. Data citation, like the citation     of other evidence and sources, is good research practice and is part of the scholarly ecosystem     supporting data reuse.In support of this assertion, and to encourage good practice, we offer a set of guiding principles     for data within scholarly literature, another dataset, or any other research object.}
}

@article{Dewald1986-ni,
  title = {Replication in {{Empirical Economics}}: {{The Journal}} of {{Money}}, {{Credit}} and {{Banking Project}}},
  author = {Dewald, William G and Thursby, Jerry G and Anderson, Richard G},
  year = {1986},
  journal = {The American Economic Review},
  volume = {76},
  number = {4},
  eprint = {1806061},
  eprinttype = {jstor},
  pages = {587--603},
  issn = {0002-8282},
  url = {https://www.jstor.org/stable/1806061}
}

@article{donoghueTeachingCreativePractical2021,
  title = {Teaching {{Creative}} and {{Practical Data Science}} at {{Scale}}},
  author = {Donoghue, Thomas and Voytek, Bradley and Ellis, Shannon E.},
  year = {2021},
  month = mar,
  journal = {Journal of Statistics and Data Science Education},
  volume = {29},
  number = {sup1},
  pages = {S27-S39},
  publisher = {Taylor \& Francis},
  issn = {null},
  doi = {10.1080/10691898.2020.1860725},
  url = {https://doi.org/10.1080/10691898.2020.1860725},
  urldate = {2021-05-25},
  abstract = {--Nolan and Temple Lang's Computing in the Statistics Curricula (2010) advocated for a shift in statistical education to broadly include computing. In the time since, individuals with training in both computing and statistics have become increasingly employable in the burgeoning data science field. In response, universities have developed new courses and programs to meet the growing demand for data science education. To address this demand, we created Data Science in Practice, a large-enrollment undergraduate course. Here, we present our goals for teaching this course, including: (1) conceptualizing data science as creative problem solving, with a focus on project-based learning, (2) prioritizing practical application, teaching and using standardized tools and best practices, and (3) scaling education through coursework that enables hands-on and classroom learning in a large-enrollment course. Throughout this course we also emphasize social context and data ethics to best prepare students for the interdisciplinary and impactful nature of their work. We highlight creative problem solving and strategies for teaching automation-resilient skills, while providing students the opportunity to create a unique data science project that demonstrates their technical and creative capacities.},
  keywords = {Computing,Course design,Data science,Programming,Project-based learning,Python},
  file = {/home/vilhuber/Zotero/storage/WPAGQ4X7/Donoghue et al. - 2021 - Teaching Creative and Practical Data Science at Sc.pdf;/home/vilhuber/Zotero/storage/RXJF2SQA/10691898.2020.html}
}

@article{editorsSupportingComputationalReproducibility2021,
  title = {Supporting Computational Reproducibility through Code Review},
  author = {{Editors}},
  year = {2021},
  month = aug,
  journal = {Nature Human Behaviour},
  volume = {5},
  number = {8},
  pages = {965--966},
  issn = {2397-3374},
  doi = {10.1038/s41562-021-01190-w},
  url = {https://www.nature.com/articles/s41562-021-01190-w},
  urldate = {2021-08-21},
  langid = {english}
}

@misc{executablebookscommunityJupyterBook2020,
  title = {Jupyter {{Book}}},
  author = {{Executable Books Community}},
  year = {2020},
  month = feb,
  doi = {10.5281/ZENODO.2561065},
  url = {https://zenodo.org/record/2561065},
  urldate = {2023-04-24},
  abstract = {{$<$}strong{$>$}Jupyter Book{$<$}/strong{$>$} is an open source project for building beautiful, publication-quality books and documents from computational material. Here are some of the features of Jupyter Book: \ding{52} Write publication-quality content in Markdown. You can write in either Jupyter Markdown, or an extended flavor of Markdown with publishing features. This includes support for rich syntax such as citations and cross-references, math and equations, and figures. \ding{52} Write content in Jupyter Notebook. This allows you to include your code and outputs in your book. You can also write notebooks entirely in Markdown that get executed when you build your book. \ding{52} Execute and cache your book's content. For {$<$}code{$>$}.ipynb{$<$}/code{$>$} and Markdown notebooks, execute code and insert the latest outputs into your book. In addition, cache and re-use outputs to be used later. \ding{52} Insert notebook outputs into your content. Generate outputs as you build your documentation, and insert them in-line with your content across pages. \ding{52} Add interactivity to your book. You can toggle cell visibility, include interactive outputs from Jupyter, and connect with online services like Binder. \ding{52} Generate a variety of outputs. This includes single- and multi-page websites, as well as PDF outputs. \ding{52} Build books with a simple command-line interface. You can quickly generate your books with one command, like so: {$<$}code{$>$}jupyter-book build mybook/{$<$}/code{$>$}},
  copyright = {Open Access},
  howpublished = {Zenodo},
  keywords = {data science,jupyter,publishing,scholarship}
}

@article{FanelliPNAS2018,
  title = {Opinion: {{Is}} Science Really Facing a Reproducibility Crisis, and Do We Need It To?},
  shorttitle = {Opinion},
  author = {Fanelli, Daniele},
  year = {2018},
  month = mar,
  journal = {Proceedings of the National Academy of Sciences},
  volume = {115},
  number = {11},
  pages = {2628--2631},
  issn = {0027-8424, 1091-6490},
  doi = {10.1073/pnas.1708272114},
  url = {http://www.pnas.org/content/115/11/2628},
  urldate = {2018-05-31},
  abstract = {Efforts to improve the reproducibility and integrity of science are typically justified by a narrative of crisis, according to which most published results are unreliable due to growing problems with research and publication practices. This article provides an overview of recent evidence suggesting that this narrative is mistaken, and argues that a narrative of epochal changes and empowerment of scientists would be more accurate, inspiring, and compelling.},
  copyright = {{\copyright} 2018 . Published under the PNAS license.},
  langid = {english},
  pmid = {29531051},
  keywords = {bias,crisis,integrity,misconduct,reproducible research},
  file = {/home/vilhuber/Zotero/storage/UKZKA4LE/Fanelli_2018_Opinion.pdf;/home/vilhuber/Zotero/storage/L9EZ577Z/2628.html}
}

@article{finebergHighlightsUSNational2020,
  title = {Highlights of the {{US National Academies Report}} on ``{{Reproducibility}} and {{Replicability}} in {{Science}}''},
  author = {Fineberg, Harvey and Stodden, Victoria and Meng, Xiao-Li},
  year = {2020},
  month = oct,
  journal = {Harvard Data Science Review},
  volume = {2},
  number = {4},
  publisher = {PubPub},
  issn = {,},
  doi = {10.1162/99608f92.cb310198},
  url = {https://hdsr.mitpress.mit.edu/pub/6an6ppum/release/4},
  urldate = {2021-08-22},
  abstract = {In 2019, the National Academies of Sciences, Engineering, and Medicine of the United States released a report on reproducibility and replicability in science. This topic is of keen interest to everyone concerned with the reliability of scientific research and the role of computational and statistical analysis in science. In this interview conducted by Xiao-Li Meng, (HDSR's Editor-in-Chief), report committee chair Harvey Fineberg (President of the Gordon and Betty Moore Foundation) and committee member Victoria Stodden (Associate Professor in the School of Information Sciences at the University of Illinois, Urbana-Champaign) recount the aims and deliberations of the committee, its major recommendations, and calls for concerted efforts from data scientists, research scientists, funding agencies, academic institutions, professional journals, and journalists to ensure scientific rigor and public trust in science.},
  langid = {english},
  file = {/home/vilhuber/Zotero/storage/TLZQCQLU/Fineberg et al_2020_Highlights of the US National Academies Report on “Reproducibility and.pdf;/home/vilhuber/Zotero/storage/SCPLTGWE/4.html}
}

@article{fortTechnologyProductionFragmentation2016,
  title = {Technology and {{Production Fragmentation}}: {{Domestic}} versus {{Foreign Sourcing}}},
  shorttitle = {Technology and {{Production Fragmentation}}},
  author = {Fort, Teresa C.},
  year = {2016},
  month = oct,
  journal = {The Review of Economic Studies},
  pages = {rdw057},
  issn = {0034-6527, 1467-937X},
  doi = {10.1093/restud/rdw057},
  url = {https://academic.oup.com/restud/article-lookup/doi/10.1093/restud/rdw057},
  urldate = {2021-09-15},
  langid = {english},
  file = {/home/vilhuber/Zotero/storage/U3YBYVB2/Fort - 2016 - Technology and Production Fragmentation Domestic .pdf}
}

@article{gallCredibilityCrisisResearch2017,
  title = {The Credibility Crisis in Research: {{Can}} Economics Tools Help?},
  shorttitle = {The Credibility Crisis in Research},
  author = {Gall, Thomas and Ioannidis, John P. A. and Maniadis, Zacharias},
  year = {2017},
  month = apr,
  journal = {PLOS Biology},
  volume = {15},
  number = {4},
  pages = {e2001846},
  issn = {1545-7885},
  doi = {10.1371/journal.pbio.2001846},
  url = {http://journals.plos.org/plosbiology/article?id=10.1371/journal.pbio.2001846},
  urldate = {2017-12-22},
  abstract = {The issue of nonreplicable evidence has attracted considerable attention across biomedical and other sciences. This concern is accompanied by an increasing interest in reforming research incentives and practices. How to optimally perform these reforms is a scientific problem in itself, and economics has several scientific methods that can help evaluate research reforms. Here, we review these methods and show their potential. Prominent among them are mathematical modeling and laboratory experiments that constitute affordable ways to approximate the effects of policies with wide-ranging implications.},
  keywords = {Behavior,Economics,Experimental economics,Health economics,Labor economics,Mathematical modeling,Peer review,Randomized controlled trials},
  file = {/home/vilhuber/Zotero/storage/FPMGIX22/Gall et al_2017_The credibility crisis in research.pdf;/home/vilhuber/Zotero/storage/CKAYM2H7/Gall et al. - 2017 - The credibility crisis in research Can economics .html}
}

@article{HamermeshAm.Econ.Rev.2017,
  title = {Replication in {{Labor Economics}}: {{Evidence}} from {{Data}}, and {{What It Suggests}}},
  shorttitle = {Replication in {{Labor Economics}}},
  author = {Hamermesh, Daniel S.},
  year = {2017},
  month = may,
  journal = {American Economic Review},
  volume = {107},
  number = {5},
  pages = {37--40},
  issn = {0002-8282},
  doi = {10.1257/aer.p20171121},
  url = {https://www.aeaweb.org/articles?id=10.1257/aer.p20171121},
  urldate = {2018-05-22},
  abstract = {Examining the most heavily cited publications in labor economics from the early 1990s, I show that few of over 3,000 articles, citing them directly, replicates them. They are replicated more frequently using data from other time periods and economies, so that the validity of their central ideas has typically been verified. This pattern of scholarship suggests, beyond the currently required depositing of data and code upon publication, that there is little need for formal mechanisms for replication. The market for scholarship already produces replications of non-laboratory applied research.},
  langid = {english},
  file = {/home/vilhuber/Zotero/storage/F6C9HVSV/articles.html}
}

@article{hamermeshViewpointReplicationEconomics2007,
  title = {Viewpoint: {{Replication}} in Economics},
  shorttitle = {Viewpoint},
  author = {Hamermesh, Daniel S.},
  year = {2007},
  month = aug,
  journal = {Canadian Journal of Economics},
  volume = {40},
  number = {3},
  pages = {715--733},
  issn = {1540-5982},
  doi = {10.1111/j.1365-2966.2007.00428.x},
  url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/j.1365-2966.2007.00428.x},
  urldate = {2018-05-21},
  abstract = {Abstract. This examination of the role and potential for replication in economics points out the paucity of both pure replication -- checking on others' published papers using their data -- and scientific replication -- using data representing different populations in one's own work or in a comment. Several controversies in empirical economics are used to illustrate how and how not to behave when replicating others' work. The incentives for replication are examined, and proposals aimed at journal editors and authors are advanced that might stimulate an activity that most economists applaud but few perform.},
  langid = {english},
  keywords = {A14,B41,C59},
  file = {/home/vilhuber/Zotero/storage/SME5DLGV/Hamermesh_2007_Viewpoint.pdf;/home/vilhuber/Zotero/storage/38IXDHC4/j.1365-2966.2007.00428.html}
}

@article{hardwickeMappingUniverseRegistered2018,
  title = {Mapping the Universe of Registered Reports},
  author = {Hardwicke, Tom E. and Ioannidis, John P. A.},
  year = {2018},
  month = nov,
  journal = {Nature Human Behaviour},
  volume = {2},
  number = {11},
  pages = {793--796},
  publisher = {Nature Publishing Group},
  issn = {2397-3374},
  doi = {10.1038/s41562-018-0444-y},
  url = {https://doi.org/10.1038/s41562-018-0444-y},
  abstract = {Registered reports present a substantial departure from traditional publishing models with the goal of enhancing the transparency and credibility of the scientific literature. We map the evolving universe of registered reports to assess their growth, implementation and shortcomings at journals across scientific disciplines.},
  langid = {english},
  file = {/home/vilhuber/Zotero/storage/3X9UMT9M/Hardwicke_Ioannidis_2018_Mapping the universe of registered reports.pdf;/home/vilhuber/Zotero/storage/2FN6LMP7/s41562-018-0444-y.html}
}

@article{herbertReproduceValidateComprehensive2024,
  title = {Reproduce to Validate: {{A}} Comprehensive Study on the Reproducibility of Economics Research},
  shorttitle = {Reproduce to Validate},
  author = {Herbert, Sylv{\'e}rie and Kingi, Hautahi and Stanchi, Flavio and Vilhuber, Lars},
  year = {2024},
  month = aug,
  journal = {Canadian Journal of Economics/Revue canadienne d'{\'e}conomique},
  pages = {caje.12728},
  issn = {0008-4085, 1540-5982},
  doi = {10.1111/caje.12728},
  url = {https://onlinelibrary.wiley.com/doi/10.1111/caje.12728},
  urldate = {2024-08-05},
  abstract = {Abstract             Journals have pushed for transparency of research through data availability policies. Such data policies improve availability of data and code, but what is the impact on reproducibility? We present results from a large reproduction exercise for articles published in the American Economic Journal: Applied Economics, which has had a data availability policy since its inception in 2009. Out of 363 published articles, we assessed 274 articles. All articles provided some materials. We excluded 122 articles that required confidential or proprietary data or that required the replicator to otherwise obtain the data (44.5\% of assessed articles). We attempted to reproduce 152 articles and were able to fully reproduce the results of 68 (44.7\% of attempted reproductions). A further 66 (43.4\% of attempted reproductions) were partially reproduced. Many articles required complex code changes even when at least partially reproduced. We collect bibliometric characteristics of authors, but find no evidence for author characteristics as determinants of reproducibility. There does not appear to be a citation bonus for reproducibility. The data availability policy of this journal was effective to ensure availability of materials, but is insufficient to ensure reproduction without additional work by replicators.           ,              R{\'e}sum{\'e}                            Les journaux militent pour la transparence de la recherche par le biais de politiques sur la disponibilit{\'e} des donn{\'e}es               . De telles politiques sur les donn{\'e}es am{\'e}liorent la disponibilit{\'e} des donn{\'e}es et du code, mais quelle en est l'incidence sur la reproductibilit{\'e}? Nous pr{\'e}sentons les r{\'e}sultats d'un grand exercice de reproduction pour des articles publi{\'e}s dans le               American Economic Journal: Applied Economics               , qui a une politique de disponibilit{\'e} des donn{\'e}es depuis sa cr{\'e}ation en 2009. Parmi les 363 articles publi{\'e}s, nous en {\'e}valuons 274. Tous les articles avaient fourni certains documents. Nous avons exclu 122 articles qui n{\'e}cessitaient des donn{\'e}es confidentielles ou exclusives, ou qui exigeaient que la personne charg{\'e}e de la r{\'e}plication obtienne autrement les donn{\'e}es (44,5\% des articles {\'e}valu{\'e}s). Nous avons tent{\'e} de reproduire 152 articles et avons r{\'e}ussi {\`a} enti{\`e}rement reproduire les r{\'e}sultats de 68 articles (44,7\% des reproductions tent{\'e}es). Nous avons aussi partiellement reproduit 66 autres articles (43,4\% des reproductions tent{\'e}es). De nombreux articles n{\'e}cessitaient de complexes changements de code, m{\^e}me lorsqu'ils {\'e}taient au moins partiellement reproduits. Nous avons recueilli les caract{\'e}ristiques bibliom{\'e}triques des auteurs, mais n'avons pas constat{\'e} qu'elles {\'e}taient d{\'e}terminantes pour la reproductibilit{\'e}. La reproductibilit{\'e} ne semble offrir aucun avantage en mati{\`e}re de citations. La politique de disponibilit{\'e} des donn{\'e}es de ce journal a {\'e}t{\'e} efficace pour assurer la disponibilit{\'e} des documents, mais insuffisante pour assurer la reproduction sans travail suppl{\'e}mentaire des personnes charg{\'e}es de la r{\'e}plication.},
  copyright = {All rights reserved},
  langid = {english},
  keywords = {Journal Policies,Replicability,Replication,Reproducibility,Transparency}
}

@article{herouxEditorialACMTOMS2015,
  title = {Editorial: {{ACM TOMS Replicated Computational Results Initiative}}},
  shorttitle = {Editorial},
  author = {Heroux, Michael A.},
  year = {2015},
  month = jun,
  journal = {ACM Transactions on Mathematical Software},
  volume = {41},
  number = {3},
  pages = {1--5},
  issn = {0098-3500, 1557-7295},
  doi = {10.1145/2743015},
  url = {https://dl.acm.org/doi/10.1145/2743015},
  urldate = {2022-02-13},
  abstract = {The scientific community relies on the peer review process for assuring the quality of published material, the goal of which is to build a body of work we can trust. Computational journals such as the               ACM Transactions on Mathematical Software               (TOMS) use this process for rigorously promoting the clarity and completeness of content, and citation of prior work. At the same time, it is unusual to independently confirm computational results.                                         ACM TOMS has established a               Replicated Computational Results               (RCR) review process as part of the manuscript peer review process. The purpose is to provide independent confirmation that results contained in a manuscript are replicable. Successful completion of the RCR process awards a manuscript with the Replicated Computational Results Designation.                          This issue of ACM TOMS contains the first [Van Zee and van de Geijn 2015] of what we anticipate to be a growing number of articles to receive the RCR designation, and the related RCR reviewer report [Willenbring 2015]. We hope that the TOMS RCR process will serve as a model for other publications and increase the confidence in and value of computational results in TOMS articles.},
  langid = {english},
  file = {/home/vilhuber/Zotero/storage/IJ598IEG/Heroux - 2015 - Editorial ACM TOMS Replicated Computational Resul.pdf}
}

@article{HofflerAm.Econ.Rev.2017,
  title = {Replication and {{Economics Journal Policies}}},
  author = {H{\"o}ffler, Jan H.},
  year = {2017},
  month = may,
  journal = {American Economic Review},
  volume = {107},
  number = {5},
  pages = {52--55},
  issn = {0002-8282},
  doi = {10.1257/aer.p20171032},
  url = {https://www.aeaweb.org/articles?id=10.1257/aer.p20171032},
  urldate = {2018-05-22},
  abstract = {Economics journals with reproducibility policies are cited more often than others. For the minority of journals with a mandatory and enforced policy, this is significant when controlling for time and journal effects. To cope with the large variety of software used and to develop standards for replicability, joint efforts of journals could ensure each empirical study is published with data, code, and instructions on how to use them together. Individual reviewers could take initiative by asking for replicable empirical results. The American Journal of Political Science sets an example by having all empirical studies externally check for replicability prior to publication.},
  langid = {english},
  file = {/home/vilhuber/Zotero/storage/B9ISM3ZZ/articles.html}
}

@article{hofflerReplicationWikiImprovingTransparency2017,
  title = {{{ReplicationWiki}}: {{Improving Transparency}} in {{Social Sciences Research}}},
  shorttitle = {{{ReplicationWiki}}},
  author = {H{\"o}ffler, Jan H.},
  year = {2017},
  month = mar,
  journal = {D-Lib Magazine},
  volume = {23},
  number = {3/4},
  issn = {1082-9873},
  doi = {10.1045/march2017-hoeffler},
  url = {http://www.dlib.org/dlib/march17/hoeffler/03hoeffler.html},
  urldate = {2018-07-17},
  langid = {english}
}

@article{hortonIntegratingComputingStatistics2021,
  title = {Integrating {{Computing}} in the {{Statistics}} and {{Data Science Curriculum}}: {{Creative Structures}}, {{Novel Skills}} and {{Habits}}, and {{Ways}} to {{Teach Computational Thinking}}},
  shorttitle = {Integrating {{Computing}} in the {{Statistics}} and {{Data Science Curriculum}}},
  author = {Horton, Nicholas J. and Hardin, Johanna S.},
  year = {2021},
  month = mar,
  journal = {Journal of Statistics and Data Science Education},
  volume = {29},
  number = {sup1},
  pages = {S1-S3},
  publisher = {Taylor \& Francis},
  issn = {null},
  doi = {10.1080/10691898.2020.1870416},
  url = {https://doi.org/10.1080/10691898.2020.1870416},
  urldate = {2021-05-25},
  abstract = {Nolan and Temple Lang argued for the fundamental role of computing in the statistics curriculum. In the intervening decade, the statistics education community has acknowledged that computational skills are as important to statistics and data science practice as mathematics. There remains a notable gap, however, between our intentions and our actions. In this special issue of the Journal of Statistics and Data Science Education, we have assembled a collection of articles that (1) suggest creative structures to integrate computing, (2) describe novel data science skills and habits, and (3) propose ways to teach computational thinking. We believe that it is critical for the community to redouble our efforts to embrace sophisticated computing in the statistics and data science curriculum. We hope that these articles provide useful guidance for the community to move these efforts forward.},
  keywords = {Algorithmic thinking,Data acumen,Education,Statistical analysis,Statistical computing,Workflow},
  file = {/home/vilhuber/Zotero/storage/L44CKDIP/Horton and Hardin - 2021 - Integrating Computing in the Statistics and Data S.pdf;/home/vilhuber/Zotero/storage/YG7QZSCW/10691898.2020.html}
}

@misc{JacobyInsideHigherEd2017,
  title = {Should {{Journals Be Responsible}} for {{Reproducibility}}?},
  shorttitle = {Should {{Journals Be Responsible}} for {{Reproducibility}}?},
  author = {Jacoby, William G. and {Lafferty-Hess}, Sophia and Christian, Thu-Mai},
  year = {2017},
  month = jul,
  journal = {Inside Higher Ed},
  url = {https://www.insidehighered.com/blogs/rethinking-research/should-journals-be-responsible-reproducibility},
  urldate = {2018-07-22},
  abstract = {One of the top journals in political science makes data-sharing and replication part of the publication process.},
  langid = {english},
  file = {/home/vilhuber/Zotero/storage/3KF4G6KZ/should-journals-be-responsible-reproducibility.html}
}

@misc{journalofdevelopmenteconomicsRegisteredReportsJDE2019,
  title = {Registered {{Reports}} at {{JDE}}: {{Lessons}} Learned so Far},
  author = {{Journal of Development Economics}},
  year = {2019},
  month = jul,
  url = {https://www.journals.elsevier.com/journal-of-development-economics/announcements/registered-reports-at-jde},
  urldate = {2020-06-15},
  abstract = {In March 2018, the\&nbsp;Journal of Development Economics\&nbsp;(JDE)\&nbsp;began piloting\&nbsp;Pre-results Review track (also referred to as \&ldquo....},
  file = {/home/vilhuber/Zotero/storage/FNCNMIFT/registered-reports-at-jde.html}
}

@article{journaloffinanceRetractedRiskManagement2021,
  title = {Retracted: {{Risk Management}} in {{Financial Institutions}}},
  shorttitle = {Retracted},
  author = {{Journal of Finance}},
  year = {2021},
  journal = {The Journal of Finance},
  volume = {n/a},
  number = {n/a},
  issn = {1540-6261},
  doi = {10.1111/jofi.13064},
  url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/jofi.13064},
  urldate = {2021-08-22},
  langid = {english},
  file = {/home/vilhuber/Zotero/storage/LPGJCY8L/Journal of Finance_2021_Retracted.pdf;/home/vilhuber/Zotero/storage/JFVEH54H/jofi.html}
}

@article{kimEasyUseCloudComputing2021,
  title = {Easy-to-{{Use Cloud Computing}} for {{Teaching Data Science}}},
  author = {Kim, Brian and Henke, Graham},
  year = {2021},
  month = mar,
  journal = {Journal of Statistics and Data Science Education},
  volume = {29},
  number = {sup1},
  pages = {S103-S111},
  publisher = {Taylor \& Francis},
  issn = {null},
  doi = {10.1080/10691898.2020.1860726},
  url = {https://doi.org/10.1080/10691898.2020.1860726},
  urldate = {2021-05-25},
  abstract = {One of the biggest hurdles of teaching data science and programming techniques to beginners is simply getting started with the technology. With multiple versions of the same coding language available (e.g., Python 2 and Python 3), various additional libraries and packages to install, as well as integrated development environments to navigate, the first step can be the most daunting. We show the advantages of using cloud computing to solve this issue and demonstrate one way of implementing it to allow beginners to get started with coding immediately. Using user-friendly Jupyter notebooks along with the interactive capabilities possible through Binder, we provide introductory Python and SQL material that students can access without downloading anything. This lets students to get started with coding right away without getting frustrated figuring out what to install. Example introductory modules on using Python and SQL for data analysis are provided through GitHub at https://github.com/Coleridge-Initiative/ada-intro-python and https://github.com/Coleridge-Initiative/ada-intro-sql.},
  keywords = {Education,Jupyter,Python,SQL,Training},
  file = {/home/vilhuber/Zotero/storage/2ID2A8PH/Kim and Henke - 2021 - Easy-to-Use Cloud Computing for Teaching Data Scie.pdf;/home/vilhuber/Zotero/storage/R7GUCNMT/10691898.2020.html}
}

@article{King1995-pg,
  title = {Replication, {{Replication}}},
  author = {King, Gary},
  year = {1995},
  month = sep,
  journal = {PS, political science \& politics},
  volume = {28},
  number = {3},
  pages = {443--499},
  issn = {1049-0965},
  abstract = {Political science is a community enterprise and the community of empirical political scientists need access to the body of data necessary to replicate existing studies to understand, evaluate, and especially build on this work. Unfortunately, the norms we have in place now do not encourage, or in some cases even permit, this aim. Following are suggestions that would facilitate replication and are easy to implement by teachers, students, dissertation writers, graduate programs, authors, reviewers, funding agencies, and journal and book editors.}
}

@techreport{Kingi2018,
  type = {Presentation},
  title = {The {{Reproducibility}} of {{Economics Research}}:  {{A Case Study}}},
  author = {Kingi, Hautahi and Stanchi, Flavio and Vilhuber, Lars and Herbert, Sylverie},
  year = {2018},
  address = {Berkeley, CA},
  url = {https://osf.io/srg57/},
  copyright = {All rights reserved}
}

@article{koenkerReproducibleEconometricResearch2009,
  title = {On Reproducible Econometric Research},
  author = {Koenker, Roger and Zeileis, Achim},
  year = {2009},
  month = aug,
  journal = {Journal of Applied Econometrics},
  volume = {24},
  number = {5},
  pages = {833--847},
  issn = {1099-1255},
  doi = {10.1002/jae.1083},
  url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/jae.1083},
  urldate = {2018-05-20},
  abstract = {Recent software developments are reviewed from the vantage point of reproducible econometric research. We argue that the emergence of new tools, particularly in the open-source community, have greatly eased the burden of documenting and archiving both empirical and simulation work in econometrics. Some of these tools are highlighted in the discussion of two small replication exercises. Copyright {\copyright} 2009 John Wiley \& Sons, Ltd.},
  copyright = {Copyright {\copyright} 2009 John Wiley \& Sons, Ltd.},
  langid = {english},
  file = {/home/vilhuber/Zotero/storage/J2495EKL/jae.html}
}

@article{macdonaldOpenDataCode2023,
  title = {Open {{Data}} and {{Code}} at the {{Urban Institute}}},
  author = {MacDonald, Graham},
  year = {2023},
  month = jul,
  journal = {Harvard Data Science Review},
  volume = {5},
  number = {3},
  issn = {2644-2353, 688-8513},
  doi = {10.1162/99608f92.a631dfc5},
  url = {https://hdsr.mitpress.mit.edu/pub/qhszdvhn/release/1},
  urldate = {2023-08-02},
  langid = {english},
  file = {/home/vilhuber/Zotero/storage/GTSZSSTZ/MacDonald_2023_Open Data and Code at the Urban Institute.pdf}
}

@article{mcculloughNumericalReliabilityEconometric1999,
  title = {The {{Numerical Reliability}} of {{Econometric Software}}},
  author = {McCullough, B. D. and Vinod, H. D.},
  year = {1999},
  month = jun,
  journal = {Journal of Economic Literature},
  volume = {37},
  number = {2},
  pages = {633--665},
  issn = {0022-0515},
  doi = {10.1257/jel.37.2.633},
  url = {https://www.aeaweb.org/articles?id=10.1257/jel.37.2.633},
  urldate = {2019-01-28},
  abstract = {Numerous examples show that some econometric software packages contain serious flaws, and that users cannot safely assume that their software is accurate. A brief survey of the fundamentals of computer arithmetic discusses the sources of numerical error and emphasizes that computer arithmetic is not at all like pencil-and-paper arithmetic. Both users and developers of econometrics software should first pay attention to accuracy, and only later consider user-friendliness. Details are provided for assessing the accuracy of basic estimation routines, statistical distributions, and random number generators. More accuracy benchmarks are needed, especially for specialized econometric procedures.},
  langid = {english},
  keywords = {Econometric Software},
  file = {/home/vilhuber/Zotero/storage/84IZAEJ3/McCullough_Vinod_1999_The Numerical Reliability of Econometric Software.pdf;/home/vilhuber/Zotero/storage/TKTKNQQ3/articles.html}
}

@article{mendez-carbajoDataCitationsReproducibility2023,
  title = {Data {{Citations}} and {{Reproducibility}} in the {{Undergraduate Curriculum}}},
  author = {{Mendez-Carbajo}, Diego and Dellachiesa, Alejandro},
  year = {2023},
  month = jul,
  journal = {Harvard Data Science Review},
  volume = {5},
  number = {3},
  issn = {2644-2353, 688-8513},
  doi = {10.1162/99608f92.c2835391},
  url = {https://hdsr.mitpress.mit.edu/pub/ohexlllx/release/1},
  urldate = {2023-08-02},
  langid = {english},
  file = {/home/vilhuber/Zotero/storage/Q9N65QIA/Mendez-Carbajo_Dellachiesa_2023_Data Citations and Reproducibility in the Undergraduate Curriculum.pdf}
}

@book{national_academies_of_sciences_data_2018,
  title = {Data {{Science}} for {{Undergraduates}}: {{Opportunities}} and {{Options}}},
  shorttitle = {Data {{Science}} for {{Undergraduates}}},
  author = {{National Academies of Sciences, Engineering, and Medicine}},
  year = {2018},
  month = may,
  doi = {10.17226/25104},
  url = {https://www.nap.edu/catalog/25104/data-science-for-undergraduates-opportunities-and-options},
  urldate = {2021-07-05},
  abstract = {Download a PDF of "Data Science for Undergraduates" by the National Academies of Sciences, Engineering, and Medicine for free.},
  isbn = {978-0-309-47559-4},
  langid = {english},
  file = {/home/vilhuber/Zotero/storage/QHC422FM/data-science-for-undergraduates-opportunities-and-options.html}
}

@book{nationalacademiesofsciencesengineeringandmedicineReproducibilityReplicabilityScience2019,
  title = {Reproducibility and {{Replicability}} in {{Science}}},
  author = {{National Academies of Sciences, Engineering, and Medicine}},
  year = {2019},
  publisher = {National Academies Press},
  address = {Washington, D.C.},
  doi = {10.17226/25303},
  url = {https://doi.org/10.17226/25303},
  urldate = {2019-09-21},
  isbn = {978-0-309-48616-3}
}

@article{nosekPreregistrationHardWorthwhile2019,
  title = {Preregistration {{Is Hard}}, {{And Worthwhile}}},
  author = {Nosek, Brian A. and Beck, Emorie D. and Campbell, Lorne and Flake, Jessica K. and Hardwicke, Tom E. and Mellor, David T. and van 't Veer, Anna E. and Vazire, Simine},
  year = {2019},
  month = oct,
  journal = {Trends in Cognitive Sciences},
  volume = {23},
  number = {10},
  pages = {815--818},
  publisher = {Elsevier},
  issn = {1364-6613, 1879-307X},
  doi = {10.1016/j.tics.2019.07.009},
  url = {https://www.cell.com/trends/cognitive-sciences/abstract/S1364-6613(19)30184-6},
  urldate = {2020-06-15},
  langid = {english},
  pmid = {31421987},
  keywords = {confirmatory research,exploratory research,preregistration,reproducibility,transparency},
  file = {/home/vilhuber/Zotero/storage/VE5MWGQA/Nosek et al_2019_Preregistration Is Hard, And Worthwhile.pdf;/home/vilhuber/Zotero/storage/4HLAR4GA/S1364-6613(19)30184-6.html}
}

@article{nosekPreregistrationRevolution2018,
  title = {The Preregistration Revolution},
  author = {Nosek, Brian A. and Ebersole, Charles R. and DeHaven, Alexander C. and Mellor, David T.},
  year = {2018},
  month = mar,
  journal = {Proceedings of the National Academy of Sciences},
  volume = {115},
  number = {11},
  pages = {2600--2606},
  issn = {0027-8424, 1091-6490},
  doi = {10.1073/pnas.1708274114},
  url = {http://www.pnas.org/lookup/doi/10.1073/pnas.1708274114},
  urldate = {2020-06-15},
  abstract = {Progress in science relies in part on generating hypotheses with existing observations and testing hypotheses with new observations. This distinction between postdiction and prediction is appreciated conceptually but is not respected in practice. Mistaking generation of postdictions with testing of predictions reduces the credibility of research findings. However, ordinary biases in human reasoning, such as hindsight bias, make it hard to avoid this mistake. An effective solution is to define the research questions and analysis plan before observing the research outcomes---a process called preregistration. Preregistration distinguishes analyses and outcomes that result from predictions from those that result from postdictions. A variety of practical strategies are available to make the best possible use of preregistration in circumstances that fall short of the ideal application, such as when the data are preexisting. Services are now available for preregistration across all disciplines, facilitating a rapid increase in the practice. Widespread adoption of preregistration will increase distinctiveness between hypothesis generation and hypothesis testing and will improve the credibility of research findings.},
  langid = {english},
  file = {/home/vilhuber/Zotero/storage/QU6BPE56/Nosek et al_2018_The preregistration revolution.pdf}
}

@article{olkenPromisesPerilsPreanalysis2015,
  ids = {olken2015},
  title = {Promises and {{Perils}} of {{Pre-analysis Plans}}},
  author = {Olken, Benjamin A.},
  year = {2015},
  month = sep,
  journal = {Journal of Economic Perspectives},
  volume = {29},
  number = {3},
  pages = {61--80},
  issn = {0895-3309},
  doi = {10.1257/jep.29.3.61},
  url = {https://www.aeaweb.org/articles?id=10.1257/jep.29.3.61},
  urldate = {2020-06-15},
  abstract = {The purpose of this paper is to help think through the advantages and costs of rigorous pre-specification of statistical analysis plans in economics.  A pre-analysis plan pre-specifies in a precise way the analysis to be run before examining the data.  A researcher can specify variables, data cleaning procedures, regression specifications, and so on.  If the regressions are pre-specified in advance and researchers are required to report all the results they pre-specify, data-mining problems are greatly reduced.  I begin by laying out the basics of what a statistical analysis plan actually contains so those researchers unfamiliar with it can better understand how it is done.  In so doing, I have drawn both on standards used in clinical trials, which are clearly specified by the Food and Drug Administration, as well as my own practical experience from writing these plans in economics contexts.  I then lay out some of the advantages of pre-specified analysis plans, both for the scientific community as a whole and also for the researcher.  I also explore some of the limitations and costs of such plans.  I then review a few pieces of evidence that suggest that, in many contexts, the benefits of using pre-specified analysis plans may not be as high as one might have expected initially.  For the most part, I will focus on the relatively narrow issue of pre-analysis for randomized controlled trials.},
  langid = {english},
  keywords = {Cluster Analysis,Factor Models,Multiple or Simultaneous Equation Models: Classification Methods,Principal Components},
  file = {/home/vilhuber/Zotero/storage/DFCV9EE3/Olken_2015_Promises and Perils of Pre-Analysis Plans.pdf;/home/vilhuber/Zotero/storage/K7ZCRP8D/Olken_2015_Promises and Perils of Pre-analysis Plans.pdf;/home/vilhuber/Zotero/storage/INJH685U/articles.html}
}

@article{peerWhyHowWe2024,
  title = {Why and {{How We Share Reproducible Research}} at {{Yale University}}'s {{Institution}} for {{Social}} and {{Policy Studies}}},
  author = {Peer, Limor},
  year = {2024},
  month = jan,
  journal = {Harvard Data Science Review},
  volume = {6},
  number = {1},
  doi = {10.1162/99608f92.dca148ba},
  url = {https://hdsr.mitpress.mit.edu/pub/lvp2cuc2},
  urldate = {2024-05-02},
  langid = {english},
  file = {/home/vilhuber/Zotero/storage/EA6IGSS4/Peer - 2024 - Why and How We Share Reproducible Research at Yale.pdf}
}

@article{perignonCertifyReproducibilityConfidential2019,
  title = {Certify Reproducibility with Confidential Data},
  author = {P{\'e}rignon, Christophe and Gadouche, Kamel and Hurlin, Christophe and Silberman, Roxane and Debonnel, Eric},
  year = {2019},
  month = jul,
  journal = {Science},
  volume = {365},
  number = {6449},
  pages = {127--128},
  issn = {0036-8075, 1095-9203},
  doi = {10.1126/science.aaw2825},
  url = {https://science.sciencemag.org/content/365/6449/127},
  urldate = {2019-09-22},
  abstract = {A trusted third party certifies that results reproduce A trusted third party certifies that results reproduce},
  copyright = {Copyright {\copyright} 2019, American Association for the Advancement of Science. http://www.sciencemag.org/about/science-licenses-journal-article-reuseThis is an article distributed under the terms of the Science Journals Default License.},
  langid = {english},
  pmid = {31296759},
  file = {/home/vilhuber/Zotero/storage/NMAWAFY5/Pérignon et al_2019_Certify reproducibility with confidential data.pdf;/home/vilhuber/Zotero/storage/FJ6WD2DT/tab-article-info.html}
}

@article{reinhartExpandingScopeStatistical2021,
  title = {Expanding the {{Scope}} of {{Statistical Computing}}: {{Training Statisticians}} to {{Be Software Engineers}}},
  shorttitle = {Expanding the {{Scope}} of {{Statistical Computing}}},
  author = {Reinhart, Alex and Genovese, Christopher R.},
  year = {2021},
  month = mar,
  journal = {Journal of Statistics and Data Science Education},
  volume = {29},
  number = {sup1},
  pages = {S7-S15},
  publisher = {Taylor \& Francis},
  issn = {null},
  doi = {10.1080/10691898.2020.1845109},
  url = {https://doi.org/10.1080/10691898.2020.1845109},
  urldate = {2021-05-25},
  abstract = {Traditionally, statistical computing courses have taught the syntax of a particular programming language or specific statistical computation methods. Since Nolan and Temple Lang's seminal paper, we have seen a greater emphasis on data wrangling, reproducible research, and visualization. This shift better prepares students for careers working with complex datasets and producing analyses for multiple audiences. But, we argue, statisticians are now often called upon to develop statistical software, not just analyses, such as R packages implementing new analysis methods or machine learning systems integrated into commercial products. This demands different skills. We describe a graduate course that we developed to meet this need by focusing on four themes: programming practices, software design, important algorithms and data structures, and essential tools and methods. Through code review and revision, and a semester-long software project, students practice all the skills of software engineering. The course allows students to expand their understanding of computing as applied to statistical problems while building expertise in the kind of software development that is increasingly the province of the working statistician. We see this as a model for the future evolution of the computing curriculum in statistics and data science.},
  keywords = {Data structures,Software engineering,Statistical computing,Version control},
  file = {/home/vilhuber/Zotero/storage/AE9MAD4L/Reinhart and Genovese - 2021 - Expanding the Scope of Statistical Computing Trai.pdf;/home/vilhuber/Zotero/storage/L7RLS62V/10691898.2020.html}
}

@misc{simonsohn98EvidenceFraud2021,
  title = {[98] {{Evidence}} of {{Fraud}} in an {{Influential Field Experiment About Dishonesty}}},
  author = {Simonsohn, Uri and Nelson, Leif and Simmons, Joe and {Anonymous}},
  year = {2021},
  month = aug,
  journal = {Data Colada},
  url = {https://datacolada.org/98},
  urldate = {2021-08-22},
  abstract = {This post is co-authored with a team of researchers who have chosen to remain anonymous. They uncovered most of the evidence reported in this post. These researchers are not connected in any way to the papers described herein. *** In 2012, Shu, Mazar, Gino, Ariely, and Bazerman published a three-study paper in PNAS (.htm) reporting...},
  langid = {american},
  keywords = {fraud,replication},
  file = {/home/vilhuber/Zotero/storage/XK49QBTH/98.html}
}

@article{stoddenEnhancingReproducibilityComputational2016,
  title = {Enhancing Reproducibility for Computational Methods},
  author = {Stodden, Victoria and McNutt, Marcia and Bailey, David H. and Deelman, Ewa and Gil, Yolanda and Hanson, Brooks and Heroux, Michael A. and Ioannidis, John P. A. and Taufer, Michela},
  year = {2016},
  month = dec,
  journal = {Science},
  volume = {354},
  number = {6317},
  pages = {1240--1241},
  issn = {0036-8075, 1095-9203},
  doi = {10.1126/science.aah6168},
  url = {http://science.sciencemag.org/content/354/6317/1240},
  urldate = {2018-05-20},
  abstract = {Over the past two decades, computational methods have radically changed the ability of researchers from all areas of scholarship to process and analyze data and to simulate complex systems. But with these advances come challenges that are contributing to broader concerns over irreproducibility in the scholarly literature, among them the lack of transparency in disclosure of computational methods. Current reporting methods are often uneven, incomplete, and still evolving. We present a novel set of Reproducibility Enhancement Principles (REP) targeting disclosure challenges involving computation. These recommendations, which build upon more general proposals from the Transparency and Openness Promotion (TOP) guidelines (1) and recommendations for field data (2), emerged from workshop discussions among funding agencies, publishers and journal editors, industry participants, and researchers representing a broad range of domains. Although some of these actions may be aspirational, we believe it is important to recognize and move toward ameliorating irreproducibility in computational research. Data, code, and workflows should be available and cited Data, code, and workflows should be available and cited},
  copyright = {Copyright {\copyright} 2016, American Association for the Advancement of Science},
  langid = {english},
  pmid = {27940837},
  file = {/home/vilhuber/Zotero/storage/T4HBYR48/1240.html}
}

@article{StoddenPNAS2018,
  title = {An Empirical Analysis of Journal Policy Effectiveness for Computational Reproducibility},
  author = {Stodden, Victoria and Seiler, Jennifer and Ma, Zhaokun},
  year = {2018},
  month = mar,
  journal = {Proceedings of the National Academy of Sciences},
  pages = {201708290},
  issn = {0027-8424, 1091-6490},
  doi = {10.1073/pnas.1708290115},
  url = {http://www.pnas.org/content/early/2018/03/08/1708290115},
  urldate = {2018-05-20},
  abstract = {A key component of scientific communication is sufficient information for other researchers in the field to reproduce published findings. For computational and data-enabled research, this has often been interpreted to mean making available the raw data from which results were generated, the computer code that generated the findings, and any additional information needed such as workflows and input parameters. Many journals are revising author guidelines to include data and code availability. This work evaluates the effectiveness of journal policy that requires the data and code necessary for reproducibility be made available postpublication by the authors upon request. We assess the effectiveness of such a policy by (i) requesting data and code from authors and (ii) attempting replication of the published findings. We chose a random sample of 204 scientific papers published in the journal Science after the implementation of their policy in February 2011. We found that we were able to obtain artifacts from 44\% of our sample and were able to reproduce the findings for 26\%. We find this policy---author remission of data and code postpublication upon request---an improvement over no policy, but currently insufficient for reproducibility.},
  copyright = {{\copyright} 2018 . Published under the PNAS license.},
  langid = {english},
  pmid = {29531050},
  keywords = {code access,data access,open science,reproducibility policy,reproducible research},
  file = {/home/vilhuber/Zotero/storage/EYXY3E2M/Stodden et al. - 2018 - An empirical analysis of journal policy effectiven.pdf;/home/vilhuber/Zotero/storage/BC3EA7VT/1708290115.html}
}

@article{vilhuberlarsTemplateREADMESocial2020,
  title = {A Template {{README}} for Social Science Replication Packages},
  author = {Vilhuber, Lars and Connolly, Marie and Koren, Mikl{\'o}s and Llull, Joan and Morrow, Peter},
  year = {2020},
  month = dec,
  publisher = {Zenodo},
  doi = {10.5281/ZENODO.4319999},
  url = {https://zenodo.org/record/4319999},
  urldate = {2021-04-01},
  abstract = {The typical README in social science journals serves the purpose of guiding a reader through the available material and a route to replicating the results in the research paper, including the description of the origins of data and/or description of programs. As such, a good README file should first provide a brief overview of the available material and a brief guide as to how to proceed from beginning to end, before then diving into the specifics. These template files structure such a README in a way that is compliant with the typical data and code workflow in the social sciences.},
  copyright = {Creative Commons Attribution Non Commercial 4.0 International, Open Access},
  langid = {english},
  keywords = {economics,reproducibility,social sciences}
}

@article{vilhuberReproducibilityReplicabilityEconomics2020,
  title = {Reproducibility and {{Replicability}} in {{Economics}}},
  author = {Vilhuber, Lars},
  year = {12 2020-12-16},
  journal = {Harvard Data Science Review},
  volume = {2},
  number = {4},
  doi = {10.1162/99608f92.4f6b9e67},
  url = {https://hdsr.mitpress.mit.edu/pub/fgpmpj1l},
  copyright = {All rights reserved}
}

@article{vilhuberTeachingLargescaleReproducibility2022,
  ids = {vilhuber2022f},
  title = {Teaching for Large-Scale {{Reproducibility Verification}}},
  author = {Vilhuber, Lars and Son, Hyuk Harry and Welch, Meredith and Wasser, David N. and Darisse, Michael},
  year = {2022},
  month = sep,
  journal = {Journal of Statistics and Data Science Education},
  volume = {30},
  number = {3},
  pages = {274--281},
  doi = {10.1080/26939169.2022.2074582},
  url = {https://arxiv.org/abs/2204.01540v1},
  abstract = {We describe a unique environment in which undergraduate students from various STEM and social science disciplines are trained in data provenance and reproducible methods, and then apply that knowledge to real, conditionally accepted manuscripts and associated replication packages. We describe in detail the recruitment, training, and regular activities. While the activity is not part of a regular curriculum, the skills and knowledge taught through explicit training of reproducible methods and principles, and reinforced through repeated application in a real-life workflow, contribute to the education of these undergraduate students, and prepare them for post-graduation jobs and further studies.},
  copyright = {All rights reserved},
  langid = {english},
  file = {/home/vilhuber/Zotero/storage/7PUR99AZ/Vilhuber et al_2022_Teaching for Large-Scale Reproducibility Verification.pdf}
}

@article{whitedCostsBenefitsReproducibility2023,
  title = {Costs and {{Benefits}} of {{Reproducibility}} in {{Finance}} and {{Economics}}},
  author = {Whited, Toni},
  year = {2023},
  month = jul,
  journal = {Harvard Data Science Review},
  volume = {5},
  number = {3},
  issn = {2644-2353, 688-8513},
  doi = {10.1162/99608f92.63de8e58},
  url = {https://hdsr.mitpress.mit.edu/pub/mnhzk8gq/release/1},
  urldate = {2023-08-02},
  langid = {english},
  file = {/home/vilhuber/Zotero/storage/2QKAE8Y7/Whited_2023_Costs and Benefits of Reproducibility in Finance and Economics.pdf}
}
